{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroTechy/CarrisInsight/blob/streaming_development/spark_jobs/extract_carris_vehicles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TputMQpEi4ax"
      },
      "source": [
        "# Step 1: Authenticate with Google Cloud\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IoSKBOhCi4a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20afdee9-25b8-4b7a-b7f5-16e862438ab6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=pL6XOyO1TgeSp7596YtHa4AcZbx5fI&prompt=consent&token_usage=remote&access_type=offline&code_challenge=ePPILVVfpRstIFVNfbMlXMj9kJ8SHDu5PpI92Mt0SOc&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AanRRrvkoqgFZqB230wZTvH46K0uKjiuUxJtymcDVx-2NL5l_lWiNxCDPQ2u0d4OOQIQ5w\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5joskWjSi4a5"
      },
      "source": [
        "# Step 2: Install Spark and BigQuery connector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark"
      ],
      "metadata": {
        "id": "Xnykj576wq14",
        "outputId": "def3ee57-27d8-4f5f-d925-421ef7bd092e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark import SparkConf\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, LongType\n",
        "from pyspark.sql.functions import min, max, first, last, col, window, from_unixtime, to_timestamp\n"
      ],
      "metadata": {
        "id": "E8AB-EuOxXAD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName('pyspark-run-with-gcp-bucket') \\\n",
        "    .config(\"spark.jars\", \"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar\") \\\n",
        "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark._jsc.hadoopConfiguration().set(\"google.cloud.auth.service.account.json.keyfile\", \"/content/.config/application_default_credentials.json\")"
      ],
      "metadata": {
        "id": "Kd0js4ULxYOE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4IR_oNvi4a9"
      },
      "source": [
        "# Step 3: Set environment variables for Spark and Java"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiCgcxzXi4bA"
      },
      "source": [
        "# Step 4: Initialize Spark session"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Ich5yIi4bA"
      },
      "source": [
        "# Step 5: Define GCS input path and output BigQuery table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BJKtwKfCi4bB"
      },
      "outputs": [],
      "source": [
        "input_path = \"gs://edit-de-project-streaming-data/carris-vehicles\"\n",
        "output_table = \"data-eng-dev-437916.data_eng_project_group3_raw\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf content/lake/processing/"
      ],
      "metadata": {
        "id": "OUX7Hw_TW02b"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SoLfFeUObaY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZiGVmPmKbaVt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo74DP8Fi4bB"
      },
      "source": [
        "# Step 6: Read streaming data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ei6YYuAei4bB"
      },
      "outputs": [],
      "source": [
        "# Define the schema for your JSON files\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"bearing\", FloatType(), True),\n",
        "    StructField(\"block_id\", StringType(), True),\n",
        "    StructField(\"current_status\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"lat\", FloatType(), True),\n",
        "    StructField(\"line_id\", StringType(), True),\n",
        "    StructField(\"lon\", FloatType(), True),\n",
        "    StructField(\"pattern_id\", StringType(), True),\n",
        "    StructField(\"route_id\", StringType(), True),\n",
        "    StructField(\"schedule_relationship\", StringType(), True),\n",
        "    StructField(\"shift_id\", StringType(), True),\n",
        "    StructField(\"speed\", FloatType(), True),\n",
        "    StructField(\"stop_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"trip_id\", StringType(), True)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def transform_data(df, batch_id):\n",
        "\n",
        "  transformed_df = df.withColumn(\"timestamp\", to_timestamp(from_unixtime(\"timestamp\"))).withWatermark(\"timestamp\", \"10 minutes\")\n",
        "  # Write a df with the datetype transform only\n",
        "  transformed_df.write.mode(\"append\").format(\"parquet\").save(\"content/lake/processing/vehicles/tests\")\n",
        "\n",
        "  window_spec = window(\"timestamp\", \"2 minutes\", \"5 seconds\")\n",
        "  # Group by vehicle ID and window, then get the first and last timestamps and lat/lon values\n",
        "  result_df = (\n",
        "      transformed_df.groupBy(\"id\", window_spec)\n",
        "      .agg(\n",
        "          min(\"timestamp\").alias(\"first_timestamp\"),\n",
        "          max(\"timestamp\").alias(\"last_timestamp\"),\n",
        "          first(\"lat\").alias(\"first_lat\"),\n",
        "          first(\"lon\").alias(\"first_lon\"),\n",
        "          last(\"lat\").alias(\"last_lat\"),\n",
        "          last(\"lon\").alias(\"last_lon\")\n",
        "      ).orderBy(\"last_timestamp\", ascending=False).dropDuplicates([\"id\"])\n",
        "      )\n",
        "  result_df.write.mode(\"append\").format(\"parquet\").save(\"content/lake/processing/vehicles/data\")\n",
        "  print(result_df.count())\n",
        "  return result_df\n",
        "\n"
      ],
      "metadata": {
        "id": "yXJ7kxEHxdw6"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "def haversine(lat1, lon1, lat2, lon2):\n",
        "    try:\n",
        "      # Earth radius in kilometers\n",
        "      R = 6371.0\n",
        "\n",
        "      # Convert latitude and longitude from degrees to radians\n",
        "      lat1_rad, lon1_rad = math.radians(lat1), math.radians(lon1)\n",
        "      lat2_rad, lon2_rad = math.radians(lat2), math.radians(lon2)\n",
        "\n",
        "      # Differences\n",
        "      delta_lat = lat2_rad - lat1_rad\n",
        "      delta_lon = lon2_rad - lon1_rad\n",
        "\n",
        "      # Haversine formula\n",
        "      a = math.sin(delta_lat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2)**2\n",
        "      c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "      # Distance\n",
        "      distance = R * c\n",
        "      return distance\n",
        "    except:\n",
        "      return 0\n",
        "\n",
        "# Example usage\n",
        "lat1, lon1 = 52.2296756, 21.0122287  # Warsaw\n",
        "lat2, lon2 = 41.8919300, 12.5113300  # Rome\n",
        "distance = haversine(lat1, lon1, lat2, lon2)"
      ],
      "metadata": {
        "id": "ZBlciteo7N2G"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mKf2yAgm7jlM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "B1MkKQ5O7jh2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark.readStream.option(\"maxFilesPerTrigger\", 1)\n",
        "    .format(\"json\")\n",
        "    .schema(schema)\n",
        "    .load(input_path))\n",
        "\n",
        "\n",
        "query = (df.writeStream\n",
        ".outputMode('append')\n",
        ".option('checkpointLocation', 'content/lake/processing/vehicles_checkpoint') #using a common checkpoint for both messages streams.\n",
        ".trigger(processingTime='5 seconds')\n",
        ".foreachBatch(transform_data)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(120)\n",
        "\n",
        "query.stop()\n"
      ],
      "metadata": {
        "id": "Ixu4EnWx3HwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "95574f75-6745-44d3-f9fa-af6073812c2a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1133\n",
            "1127\n",
            "1120\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:py4j.clientserver:There was an exception while executing the Python Proxy on the Python Side.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 617, in _call_proxy\n",
            "    return_value = getattr(self.pool[obj_id], method)(*params)\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/utils.py\", line 120, in call\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/utils.py\", line 117, in call\n",
            "    self.func(DataFrame(jdf, wrapped_session_jdf), batch_id)\n",
            "  File \"<ipython-input-25-f94e2fdfd712>\", line 20, in transform_data\n",
            "    result_df.write.mode(\"append\").format(\"parquet\").save(\"content/lake/processing/vehicles/data\")\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\", line 1463, in save\n",
            "    self._jwrite.save(path)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "                   ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "           ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\", line 326, in get_return_value\n",
            "    raise Py4JJavaError(\n",
            "py4j.protocol.Py4JJavaError: An error occurred while calling o874.save.\n",
            ": java.lang.InterruptedException\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1040)\n",
            "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1345)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:242)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)\n",
            "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:187)\n",
            "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:342)\n",
            "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:980)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
            "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
            "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
            "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
            "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
            "\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\n",
            "\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$.prepareShuffleDependency(ShuffleExchangeExec.scala:296)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:179)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
            "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
            "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
            "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
            "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
            "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n",
            "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
            "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:59)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:141)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:141)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:175)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:173)\n",
            "\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
            "\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:527)\n",
            "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:455)\n",
            "\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:454)\n",
            "\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:498)\n",
            "\tat org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:132)\n",
            "\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:751)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
            "\tat org.apache.spark.sql.execution.aggregate.SortAggregateExec.doExecute(SortAggregateExec.scala:59)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:195)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:191)\n",
            "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.doExecuteWrite(WriteFiles.scala:74)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeWrite$1(SparkPlan.scala:235)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:246)\n",
            "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:243)\n",
            "\tat org.apache.spark.sql.execution.SparkPlan.executeWrite(SparkPlan.scala:231)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:305)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
            "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
            "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
            "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
            "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
            "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n",
            "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n",
            "\tat jdk.internal.reflect.GeneratedMethodAccessor150.invoke(Unknown Source)\n",
            "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
            "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
            "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
            "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
            "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
            "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
            "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
            "\tat py4j.ClientServerConnection.sendCommand(ClientServerConnection.java:244)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:384)\n",
            "\tat py4j.CallbackClient.sendCommand(CallbackClient.java:356)\n",
            "\tat py4j.reflection.PythonProxyHandler.invoke(PythonProxyHandler.java:106)\n",
            "\tat com.sun.proxy.$Proxy34.call(Unknown Source)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.PythonForeachBatchHelper$.$anonfun$callForeachBatch$1$adapted(ForeachBatchSink.scala:53)\n",
            "\tat org.apache.spark.sql.execution.streaming.sources.ForeachBatchSink.addBatch(ForeachBatchSink.scala:34)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:732)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
            "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
            "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
            "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
            "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
            "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/processing/vehicles/data\")\n"
      ],
      "metadata": {
        "id": "SU7P7Bs26c4C"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JLG3QL4u6bcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tests = spark.read.format(\"parquet\").load(\"content/lake/processing/vehicles/tests\")\n"
      ],
      "metadata": {
        "id": "e2a6Wtry5hLA"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "1TpKBxJW75Fc",
        "outputId": "75553e73-fed3-4f30-aae6-45d9fc069cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4521"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tests.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbI63QRN_Wx6",
        "outputId": "2970aefa-ec0c-441c-84a8-3016cd479473"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12476"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a new column that applies haversine to each row to the values of columns first_lat, first_lon, last_lat and last_lon\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import FloatType,   StringType, IntegerType\n",
        "\n",
        "\n",
        "haversine_udf = F.udf(haversine, FloatType())\n",
        "\n",
        "df = df.withColumn(\"distance\", haversine_udf(F.col(\"first_lat\"), F.col(\"first_lon\"), F.col(\"last_lat\"), F.col(\"last_lon\")))\n",
        "\n",
        "# Apply the UDF to create the new column\n"
      ],
      "metadata": {
        "id": "f0h3skZHByb_"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "window_spec = window(\"timestamp\", \"2 minutes\")\n",
        "  # Group by vehicle ID and window, then get the first and last timestamps and lat/lon values\n",
        "result_df = (tests.groupBy(\"id\", window_spec)\n",
        "      .agg(\n",
        "          min(\"timestamp\").alias(\"first_timestamp\"),\n",
        "          max(\"timestamp\").alias(\"last_timestamp\"),\n",
        "          first(\"lat\").alias(\"first_lat\"),\n",
        "          first(\"lon\").alias(\"first_lon\"),\n",
        "          last(\"lat\").alias(\"last_lat\"),\n",
        "          last(\"lon\").alias(\"last_lon\")\n",
        "      ).orderBy(\"last_timestamp\", ascending=False).dropDuplicates([\"id\"]).withColumn(\"distance\", haversine_udf(F.col(\"first_lat\"), F.col(\"first_lon\"), F.col(\"last_lat\"), F.col(\"last_lon\")))\n",
        "      )"
      ],
      "metadata": {
        "id": "XM2ERwvW51a7"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.filter(df[\"first_lat\"] != df[\"last_lat\"]).show()"
      ],
      "metadata": {
        "id": "M2FedCG86dE7",
        "outputId": "e4c17524-7070-41d3-e398-d0fba968b21b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---------------+--------------+---------+---------+--------+--------+--------+\n",
            "| id|window|first_timestamp|last_timestamp|first_lat|first_lon|last_lat|last_lon|distance|\n",
            "+---+------+---------------+--------------+---------+---------+--------+--------+--------+\n",
            "+---+------+---------------+--------------+---------+---------+--------+--------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_df.filter(result_df[\"first_lat\"] != result_df[\"last_lat\"]).show()"
      ],
      "metadata": {
        "id": "MEnUJS2v4i0v",
        "outputId": "0ba32c80-2cad-4741-c2cd-2c3e52bedb5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+-------------------+-------------------+---------+---------+---------+---------+------------+\n",
            "|     id|              window|    first_timestamp|     last_timestamp|first_lat|first_lon| last_lat| last_lon|    distance|\n",
            "+-------+--------------------+-------------------+-------------------+---------+---------+---------+---------+------------+\n",
            "|41|1100|{2025-01-17 08:48...|2025-01-17 08:48:02|2025-01-17 08:48:25|38.721485|-9.202832|38.722572|-9.199898|    0.281819|\n",
            "|41|1102|{2025-01-17 08:48...|2025-01-17 08:48:10|2025-01-17 08:48:52| 38.71482|-9.241849|38.716415|-9.239246|  0.28709525|\n",
            "|41|1103|{2025-01-17 08:48...|2025-01-17 08:48:14|2025-01-17 08:48:51|38.726063|-9.238774|38.726112|-9.236004|  0.24039264|\n",
            "|41|1105|{2025-01-17 08:48...|2025-01-17 08:48:00|2025-01-17 08:48:57|38.725433|-9.310746|38.725048|-9.309947|  0.08149729|\n",
            "|41|1107|{2025-01-17 08:48...|2025-01-17 08:48:18|2025-01-17 08:48:41|38.700886|-9.227971|38.700726|-9.227794|  0.02354433|\n",
            "|41|1108|{2025-01-17 08:48...|2025-01-17 08:48:04|2025-01-17 08:48:48|38.726025|-9.244004|38.725903|-9.243074| 0.081795566|\n",
            "|41|1109|{2025-01-17 08:48...|2025-01-17 08:48:17|2025-01-17 08:48:48| 38.72402|-9.226717| 38.72274|-9.225333|  0.18601842|\n",
            "|41|1112|{2025-01-17 08:48...|2025-01-17 08:48:05|2025-01-17 08:48:35|38.726875|-9.239811|38.725853|-9.240864|  0.14582396|\n",
            "|41|1114|{2025-01-17 08:48...|2025-01-17 08:48:20|2025-01-17 08:48:47|38.723766|-9.225991|38.723927|-9.226416| 0.040899765|\n",
            "|41|1116|{2025-01-17 08:48...|2025-01-17 08:48:07|2025-01-17 08:48:53|38.726357|-9.237684|38.725796|-9.235209|  0.22355512|\n",
            "|41|1118|{2025-01-17 08:48...|2025-01-17 08:48:13|2025-01-17 08:48:44| 38.70106|-9.249801| 38.70104| -9.24972|0.0074807177|\n",
            "|41|1119|{2025-01-17 08:48...|2025-01-17 08:48:20|2025-01-17 08:48:50| 38.78324|-9.227017| 38.78266| -9.22761|  0.08241461|\n",
            "|41|1123|{2025-01-17 08:48...|2025-01-17 08:48:08|2025-01-17 08:48:25| 38.71226| -9.30173| 38.70973|-9.301816|  0.28175062|\n",
            "|41|1124|{2025-01-17 08:48...|2025-01-17 08:48:12|2025-01-17 08:48:54|38.725388| -9.24606|38.725353|-9.248302|  0.19453663|\n",
            "|41|1125|{2025-01-17 08:48...|2025-01-17 08:48:18|2025-01-17 08:48:43| 38.78302| -9.22721| 38.78283|-9.227428| 0.028428078|\n",
            "|41|1126|{2025-01-17 08:48...|2025-01-17 08:48:12|2025-01-17 08:48:32| 38.72739|-9.161511|38.727665|-9.160801|  0.06878421|\n",
            "|41|1127|{2025-01-17 08:48...|2025-01-17 08:48:12|2025-01-17 08:48:43|38.738796|-9.270144|38.738316|-9.269078|  0.10680918|\n",
            "|41|1130|{2025-01-17 08:48...|2025-01-17 08:48:20|2025-01-17 08:48:54| 38.69265|-9.311272| 38.69303|-9.310299| 0.094480455|\n",
            "|41|1133|{2025-01-17 08:48...|2025-01-17 08:48:14|2025-01-17 08:48:52| 38.70415|-9.290187|38.703594|-9.290646| 0.073618814|\n",
            "|41|1135|{2025-01-17 08:46...|2025-01-17 08:46:09|2025-01-17 08:47:56| 38.75589|-9.265734| 38.75073|-9.264243|   0.5882845|\n",
            "+-------+--------------------+-------------------+-------------------+---------+---------+---------+---------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "fV5e8MdKP1Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_f0Jzjb3P2aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8G6JQs7tCFaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNoXxTT4xOOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert the column timestamp to timestamp\n",
        "\n",
        "from pyspark.sql.functions import from_unixtime, to_timestamp\n"
      ],
      "metadata": {
        "id": "0GiwZmafzrzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: group by vehicle in 2 minutes buckets\n",
        "\n",
        "from pyspark.sql.functions import window, from_unixtime, col, to_timestamp\n",
        "\n",
        "# Assuming 'vehicles_table' is your table with the vehicle data\n",
        "\n",
        "# Read the data\n",
        "vehicle_data = spark.read.parquet(\"vehicles_table\")\n",
        "\n",
        "# Convert the timestamp to a timestamp type and adjust timezone\n",
        "vehicle_data = vehicle_data.withColumn(\"timestamp_readable\", to_timestamp(from_unixtime(col(\"timestamp\") / 1000)))\n",
        "\n",
        "# Group by vehicle and 2-minute intervals\n",
        "grouped_vehicle_data = vehicle_data.groupBy(\n",
        "    \"id\", window(\"timestamp_readable\", \"2 minutes\")\n",
        ").count()\n",
        "\n",
        "\n",
        "# Show the results\n",
        "grouped_vehicle_data.show(truncate=False)\n",
        "\n",
        "# Optional: Write the grouped data to a new location\n",
        "# grouped_vehicle_data.write.mode(\"overwrite\").parquet(\"grouped_vehicles_data\")"
      ],
      "metadata": {
        "id": "L6dm6YUuz_gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  spark.read.table(\"vehicles_table\").withColumn(\"timestamp\", to_timestamp(from_unixtime(\"timestamp\"))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o11cpIq_WvXO",
        "outputId": "2bbee929-031b-4f97-9d1e-6171a27b72d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "|bearing|            block_id|current_status|      id|      lat|line_id|      lon|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id|          timestamp|             trip_id|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "|  183.0|20250116-64010165...| IN_TRANSIT_TO|44|12745| 38.76761|   4720|-9.100393|  4720_0_1|  4720_0|            SCHEDULED|113260234560|      0.0| 060011|2025-01-16 11:16:53|4720_0_1|2700|111...|\n",
            "|   58.0|20250116-64010273...| IN_TRANSIT_TO|44|12557| 38.57138|   4641| -9.03899|  4641_0_2|  4641_0|            SCHEDULED|111500234560|      0.0| 150013|2025-01-16 11:16:43|4641_0_2|2700|102...|\n",
            "|   12.0|20250116-64010076...|    STOPPED_AT|44|12092| 38.70356|   4203|-8.953323|  4203_0_1|  4203_0|            SCHEDULED|121690234560|      0.0| 100038|2025-01-16 11:16:52|4203_0_1|2700|111...|\n",
            "|   97.0|20250116-64010042...|   INCOMING_AT|44|12746|38.701836|   4705| -8.97245|  4705_0_2|  4705_0|            SCHEDULED|123060234560|2.7777777| 100029|2025-01-16 11:16:46|4705_0_2|2700|110...|\n",
            "|  131.0|20250116-64010205...| IN_TRANSIT_TO|44|12731|38.550438|   4431|-8.882385|  4431_0_2|  4431_0|            SCHEDULED|112450234560|      0.0| 160529|2025-01-16 11:16:39|4431_0_2|2700|113...|\n",
            "|   67.0|20250116-64010146...| IN_TRANSIT_TO|44|12060|38.649464|   4604|-8.989128|  4604_0_1|  4604_0|            SCHEDULED|121010234560|      0.0| 090117|2025-01-16 11:16:41|4604_0_1|2700|104...|\n",
            "|   28.0|20250116-64010080...|   INCOMING_AT|44|12503|38.708984|   4514|-8.717798|  4514_0_1|  4514_0|            SCHEDULED|121650234560|15.833333| 130488|2025-01-16 11:16:47|4514_0_1|2700|103...|\n",
            "|  151.0|20250116-64010112...| IN_TRANSIT_TO|44|12506| 38.65653|   4600|-8.991332|  4600_0_1|  4600_0|            SCHEDULED|121340234560|      0.0| 090018|2025-01-16 11:16:45|4600_0_1|2700|103...|\n",
            "|  136.0|20250116-64010130...| IN_TRANSIT_TO|44|12515|38.720238|   4600|-8.999806|  4600_0_1|  4600_0|            SCHEDULED|121170234560|      5.0| 100283|2025-01-16 11:16:40|4600_0_1|2700|110...|\n",
            "|  200.0|20250116-64010164...| IN_TRANSIT_TO|44|12747|38.539406|   4720|-8.872938|  4720_0_1|  4720_0|            SCHEDULED|113250234560|     27.5| 160027|2025-01-16 11:16:38|4720_0_1|2700|104...|\n",
            "|  321.0|20250116-64010072...| IN_TRANSIT_TO|44|12633| 38.70059|   4513|-8.958016|  4513_0_2|  4513_0|            SCHEDULED|121720234560|13.611111| 100008|2025-01-16 11:16:51|4513_0_2|2700|110...|\n",
            "|  310.0|20250116-64010390...| IN_TRANSIT_TO|44|12742| 38.75371|   4702|-8.959135|  4702_0_1|  4702_0|            SCHEDULED|123040234560|      0.0| 010079|2025-01-16 11:16:45|4702_0_1|2700|105...|\n",
            "|   96.0|20250116-64010181...| IN_TRANSIT_TO|44|12673|38.539352|   4725| -8.88745|  4725_0_2|  4725_0|            SCHEDULED|113030234560| 9.722222| 162005|2025-01-16 11:16:41|4725_0_2|2700|111...|\n",
            "|    0.0|20250116-64010200...| IN_TRANSIT_TO|44|12085|38.522995|   4438|-8.895043|  4438_0_2|  4438_0|            SCHEDULED|112500234560|      0.0| 160105|2025-01-16 11:16:49|4438_0_2|2700|111...|\n",
            "|    0.0|       ESC_DU_EU1003|    STOPPED_AT| 43|2339|38.670578|   3505|-9.157463|  3505_0_2|  3505_0|            SCHEDULED|      EU1115|      0.0| 020533|2025-01-16 11:16:48|3505_0_2_1030_105...|\n",
            "|   83.0|20250116-64010167...| IN_TRANSIT_TO|44|12691| 38.52537|   4730|-8.887234|  4730_0_1|  4730_0|            SCHEDULED|113220234560|6.9444447| 160141|2025-01-16 11:16:41|4730_0_1|2700|100...|\n",
            "|   69.0|20250116-64010303...| IN_TRANSIT_TO|44|12586|38.580097|   4544|-8.722515|  4544_0_2|  4544_0|            SCHEDULED|111090234560|     20.0| 130630|2025-01-16 11:16:40|4544_0_2|2700|104...|\n",
            "|   25.0|           1_1739-11| IN_TRANSIT_TO| 41|1197| 38.78523|   1625|  -9.3438|  1625_1_1|  1625_1|            SCHEDULED|        1753| 9.722222| 171139|2025-01-16 11:16:39|1625_1_1_1030_105...|\n",
            "|   78.0|           1_1616-11| IN_TRANSIT_TO| 41|1315|38.756493|   1601|-9.263448|  1601_0_2|  1601_0|            SCHEDULED|        1602|12.222222| 170923|2025-01-16 11:16:43|1601_0_2_1030_105...|\n",
            "|  299.0|           1_1301-11| IN_TRANSIT_TO| 41|1400| 38.70076|   1614|-9.338386|  1614_1_1|  1614_1|            SCHEDULED|        1346|     10.0| 050163|2025-01-16 11:16:30|1614_1_1_1030_105...|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd1mGZxai4bB"
      },
      "source": [
        "# Step 7: Write streaming data to BigQuery with auto-table creation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streaming_query.stop()"
      ],
      "metadata": {
        "id": "0U3QqQwuTGVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKbyGnOSSmxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}