{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroTechy/CarrisInsight/blob/streaming_development/spark_jobs/extract_carris_vehicles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TputMQpEi4ax"
      },
      "source": [
        "# Step 1: Authenticate with Google Cloud\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IoSKBOhCi4a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e1d3339-90d3-4f6a-9b2e-a8cc615bd33a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Go to the following link in your browser, and complete the sign-in prompts:\n",
            "\n",
            "    https://accounts.google.com/o/oauth2/auth?response_type=code&client_id=764086051850-6qr4p6gpi6hn506pt8ejuq83di341hur.apps.googleusercontent.com&redirect_uri=https%3A%2F%2Fsdk.cloud.google.com%2Fapplicationdefaultauthcode.html&scope=openid+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fuserinfo.email+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fcloud-platform+https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fsqlservice.login&state=6Jpx2FkIAGx6rqiZL50YALX4pYR30u&prompt=consent&token_usage=remote&access_type=offline&code_challenge=1D7mGMuiu2RULIaB7St1v3NjLIQFOjq-82i3zUC7veA&code_challenge_method=S256\n",
            "\n",
            "Once finished, enter the verification code provided in your browser: 4/0AanRRrvfLaTrqkac_sXQx2ZH2VR_BfcBaya2Zr-32X9I9S-J-gU9MtvnfC6270C0CEl9_g\n",
            "\n",
            "Credentials saved to file: [/content/.config/application_default_credentials.json]\n",
            "\n",
            "These credentials will be used by any library that requests Application Default Credentials (ADC).\n",
            "\u001b[1;33mWARNING:\u001b[0m \n",
            "Cannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\n"
          ]
        }
      ],
      "source": [
        "!gcloud auth application-default login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5joskWjSi4a5"
      },
      "source": [
        "# Step 2: Install Spark and BigQuery connector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install OpenJDK 8\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark from a reliable source\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "\n",
        "# Extract the downloaded Spark tarball\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "\n",
        "!rm -rf spark-3.2.1-bin-hadoop3.2.tgz # No longer needed\n",
        "\n",
        "# Install the GCS Connector\n",
        "!rm -rf /content/spark-3.2.1-bin-hadoop3.2/jars/gcs-connector-hadoop3-latest.jar\n",
        "!wget -q https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop3-latest.jar -P /content/spark-3.2.1-bin-hadoop3.2/jars/\n",
        "\n",
        "# Install the BigQuery Connector\n",
        "!rm -rf /content/spark-3.2.1-bin-hadoop3.2/jars/spark-bigquery-with-dependencies_2.12-0.29.0.jar\n",
        "!wget -q https://storage.googleapis.com/spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.29.0.jar -P /content/spark-3.2.1-bin-hadoop3.2/jars/\n",
        "\n",
        "# Install Findspark\n",
        "!pip install -q findspark\n"
      ],
      "metadata": {
        "id": "DqZLhgr32g1T"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4IR_oNvi4a9"
      },
      "source": [
        "# Step 3: Set environment variables for Spark and Java"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8jGIzfwDi4a-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"/content/.config/application_default_credentials.json\"\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiCgcxzXi4bA"
      },
      "source": [
        "# Step 4: Initialize Spark session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3CgJ2RHIi4bA",
        "outputId": "f7f7ceb4-a90c-49f3-d9b7-eee02f7466b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session successfully created!\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"GCS to BigQuery Streaming\") \\\n",
        "    .config(\"spark.jars\", \"/content/spark-3.2.1-bin-hadoop3.2/jars/gcs-connector-hadoop3-latest.jar,/content/spark-3.2.1-bin-hadoop3.2/jars/spark-bigquery-with-dependencies_2.12-0.29.0.jar\") \\\n",
        "    .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark Session successfully created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27Ich5yIi4bA"
      },
      "source": [
        "# Step 5: Define GCS input path and output BigQuery table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJKtwKfCi4bB"
      },
      "outputs": [],
      "source": [
        "input_path = \"gs://edit-de-project-streaming-data/carris-vehicles\"\n",
        "output_table = \"data-eng-dev-437916.data_eng_project_group3_raw\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "SoLfFeUObaY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ZiGVmPmKbaVt"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo74DP8Fi4bB"
      },
      "source": [
        "# Step 6: Read streaming data from GCS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ei6YYuAei4bB"
      },
      "outputs": [],
      "source": [
        "# Define the schema for your JSON files\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, LongType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"bearing\", FloatType(), True),\n",
        "    StructField(\"block_id\", StringType(), True),\n",
        "    StructField(\"current_status\", StringType(), True),\n",
        "    StructField(\"id\", StringType(), True),\n",
        "    StructField(\"lat\", FloatType(), True),\n",
        "    StructField(\"line_id\", StringType(), True),\n",
        "    StructField(\"lon\", FloatType(), True),\n",
        "    StructField(\"pattern_id\", StringType(), True),\n",
        "    StructField(\"route_id\", StringType(), True),\n",
        "    StructField(\"schedule_relationship\", StringType(), True),\n",
        "    StructField(\"shift_id\", StringType(), True),\n",
        "    StructField(\"speed\", FloatType(), True),\n",
        "    StructField(\"stop_id\", StringType(), True),\n",
        "    StructField(\"timestamp\", LongType(), True),\n",
        "    StructField(\"trip_id\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create the streaming DataFrame with the defined schema\n",
        "streaming_df = spark.readStream \\\n",
        "    .format(\"json\") \\\n",
        "    .schema(schema) \\\n",
        "    .load(input_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_data(df, batch_id):\n",
        "\n",
        "  transformed_df = df.withColumn(\"timestamp\", to_timestamp(from_unixtime(\"timestamp\")))\n",
        "\n",
        "  return transformed_df.write.mode(\"append\").format(\"parquet\").save(\"content/lake/silver/vehicles/data\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "yXJ7kxEHxdw6"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def haversine(lat1, lon1, lat2, lon2): # Earth radius in kilometers R = 6371.0# Convert latitude and longitude from degrees to radians lat1_rad, lon1_rad = math.radians(lat1), math.radians(lon1) lat2_rad, lon2_rad = math.radians(lat2), math.radians(lon2) # Differences delta_lat = lat2_rad - lat1_rad delta_lon = lon2_rad - lon1_rad # Haversine formula a = math.sin(delta_lat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(delta_lon / 2)**2 c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a)) # Distance distance = R * c return distance # Example usage lat1, lon1 = 52.2296756, 21.0122287  # Warsaw lat2, lon2 = 41.8919300, 12.5113300  # Rome distance = haversine(lat1, lon1, lat2, lon2)"
      ],
      "metadata": {
        "id": "ZBlciteo7N2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = (spark.readStream.option(\"maxFilesPerTrigger\", 1)\n",
        "    .format(\"json\")\n",
        "    .schema(schema)\n",
        "    .load(input_path))\n",
        "\n",
        "\n",
        "query = (df.writeStream\n",
        ".outputMode('append')\n",
        ".option('checkpointLocation', 'content/lake/silver/vehicles_checkpoint') #using a common checkpoint for both messages streams.\n",
        ".trigger(processingTime='5 seconds')\n",
        ".foreachBatch(transform_data)\n",
        ".start()\n",
        ")"
      ],
      "metadata": {
        "id": "Ixu4EnWx3HwY"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/silver/vehicles/data\")\n"
      ],
      "metadata": {
        "id": "SU7P7Bs26c4C"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "1lhMjC4z68nL",
        "outputId": "d53f2588-5e20-4709-ab9d-33175a0112b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "|bearing|            block_id|current_status|      id|      lat|line_id|      lon|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id|          timestamp|             trip_id|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "|  261.0|20250116-64010206...| IN_TRANSIT_TO|44|12066|38.523514|   4434|-8.893788|  4434_0_1|  4434_0|            SCHEDULED|112440234560|      0.0| 160105|2025-01-16 12:16:42|4434_0_1|2700|120...|\n",
            "|  260.0|20250116-64010318...| IN_TRANSIT_TO|44|12574|38.570927|   4562|-8.885346|  4562_0_1|  4562_0|            SCHEDULED|111060234560|      0.0| 130663|2025-01-16 12:16:54|4562_0_1|2700|114...|\n",
            "|  299.0|20250116-64010197...| IN_TRANSIT_TO|44|12086|38.526676|   4441|-8.892803|  4441_0_2|  4441_0|            SCHEDULED|112530234560| 8.055555| 160099|2025-01-16 12:16:53|4441_0_2|2700|120...|\n",
            "|  107.0|20250116-64010035...| IN_TRANSIT_TO|44|12653|38.647255|   4701|-8.993425|  4701_0_2|  4701_0|            SCHEDULED|123030234560|      0.0| 090016|2025-01-16 12:16:42|4701_0_2|2700|120...|\n",
            "|   78.0|20250116-64010278...|   INCOMING_AT|44|12552|38.546036|   4901|-8.817813|  4901_0_2|  4901_0|            SCHEDULED|111450234560|      0.0| 160658|2025-01-16 12:16:52|4901_0_2|2700|120...|\n",
            "|  247.0|20250116-64010320...| IN_TRANSIT_TO|44|12559|38.545776|   4560|-9.025275|  4560_0_1|  4560_0|            SCHEDULED|111040234560|0.5555556| 160793|2025-01-16 12:16:49|4560_0_1|2700|120...|\n",
            "|  255.0|20250116-64010210...| IN_TRANSIT_TO|44|12075|38.528694|   4421|-8.908475|  4421_0_1|  4421_0|            SCHEDULED|112400234560|1.3888888| 160449|2025-01-16 12:16:41|4421_0_1|2700|113...|\n",
            "|  193.0|20250116-64010042...| IN_TRANSIT_TO|44|12746| 38.70625|   4705|-8.939857|  4705_0_1|  4705_0|            SCHEDULED|123060234560|21.666666| 100042|2025-01-16 12:16:41|4705_0_1|2700|120...|\n",
            "|  320.0|20250116-64010108...|   INCOMING_AT|44|12082| 38.58176|   4531|-8.909661|  4531_0_2|  4531_0|            SCHEDULED|121380234560|15.555555| 130620|2025-01-16 12:16:54|4531_0_2|2700|121...|\n",
            "|  238.0|20250116-64010303...| IN_TRANSIT_TO|44|12586|38.533768|   4544|-8.869741|  4544_0_1|  4544_0|            SCHEDULED|111090234560|5.5555553| 160243|2025-01-16 12:16:43|4544_0_1|2700|114...|\n",
            "|  245.0|20250116-64010145...|    STOPPED_AT|44|12501| 38.67338|   4600|-8.972571|  4600_0_1|  4600_0|            SCHEDULED|121020234560|      0.0| 100421|2025-01-16 12:16:44|4600_0_1|2700|113...|\n",
            "|  139.0|           1_1024-11| IN_TRANSIT_TO| 41|1248|38.763523|   1003|-9.229732|  1003_0_1|  1003_0|            SCHEDULED|        1088| 9.166667| 030141|2025-01-16 12:16:11|1003_0_1_1130_115...|\n",
            "|  181.0|20250116-64010136...| IN_TRANSIT_TO|44|12537| 38.65114|   4104|-8.987273|  4104_0_3|  4104_0|            SCHEDULED|121110234560|4.7222223| 090052|2025-01-16 12:16:45|4104_0_3|2700|114...|\n",
            "|   55.0|20250116-64010118...| IN_TRANSIT_TO|44|12528| 38.55497|   4612|-8.972035|  4612_0_1|  4612_0|            SCHEDULED|121280234560|      0.0| 130122|2025-01-16 12:16:47|4612_0_1|2700|114...|\n",
            "|  293.0|20250116-64010213...| IN_TRANSIT_TO|44|12065| 38.53644|   4438|-8.870905|  4438_0_1|  4438_0|            SCHEDULED|112370234560| 8.333333| 160690|2025-01-16 12:16:42|4438_0_1|2700|121...|\n",
            "|   87.0|20250116-64010126...|    STOPPED_AT|44|12512| 38.74911|   4512|-8.965905|  4512_0_2|  4512_0|            SCHEDULED|121210234560|      0.0| 010082|2025-01-16 12:16:48|4512_0_2|2700|111...|\n",
            "|  196.0|20250116-64010236...| IN_TRANSIT_TO|44|12568|38.528996|   4420|-8.886375|  4420_0_2|  4420_0|            SCHEDULED|112160234560|2.2222223| 160213|2025-01-16 12:16:44|4420_0_2|2700|121...|\n",
            "|  170.0|20250116-64010140...| IN_TRANSIT_TO|44|12637|38.653137|   4106|-9.017529|  4106_0_3|  4106_0|            SCHEDULED|121070234560|6.9444447| 090010|2025-01-16 12:16:42|4106_0_3|2700|121...|\n",
            "|  155.0|20250116-64010422...| IN_TRANSIT_TO|44|12631|38.537136|   4432|-9.000934|  4432_0_2|  4432_0|            SCHEDULED|111610234560|      0.0| 160596|2025-01-16 12:16:41|4432_0_2|2700|122...|\n",
            "|    0.0|      1_1412-11'_002|    STOPPED_AT| 41|1810|38.698456|   1124|-9.292858|  1124_0_3|  1124_0|            SCHEDULED|        1481|      0.0| 120890|2025-01-16 12:16:54|1124_0_3_1200_122...|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sWAQnx2w6t2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tempfile\n",
        "\n",
        "import time\n",
        "\n",
        "_ = spark.sql(\"DROP TABLE IF EXISTS my_table2\")\n",
        "\n",
        "\n",
        "    # Create a table with Rate source.\n",
        "    query = (spark.readStream.option(\"maxFilesPerTrigger\", 1)\n",
        "    .format(\"json\")\n",
        "    .schema(schema)\n",
        "    .load(input_path).withColumn(\"timestamp\", to_timestamp(from_unixtime(\"timestamp\")))\n",
        "    .writeStream\n",
        "             .toTable(\n",
        "\n",
        "            \"vehicles_table\",\n",
        "\n",
        "            queryName='read_query',\n",
        "\n",
        "            outputMode=\"append\",\n",
        "\n",
        "            format='parquet',\n",
        "\n",
        "            checkpointLocation=d))\n",
        "\n",
        "    time.sleep(10)\n",
        "\n",
        "    query.stop()\n",
        "\n"
      ],
      "metadata": {
        "id": "lpHO6EFuWMy6"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DNoXxTT4xOOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.table(\"vehicles_table\").show()"
      ],
      "metadata": {
        "id": "BTgBtpm6lZLk",
        "outputId": "a7c0cde6-c7b7-4334-ed3e-5e626d030770",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+----------+--------------------+\n",
            "|bearing|            block_id|current_status|      id|      lat|line_id|      lon|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id| timestamp|             trip_id|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+----------+--------------------+\n",
            "|  183.0|20250116-64010165...| IN_TRANSIT_TO|44|12745| 38.76761|   4720|-9.100393|  4720_0_1|  4720_0|            SCHEDULED|113260234560|      0.0| 060011|1737026213|4720_0_1|2700|111...|\n",
            "|   58.0|20250116-64010273...| IN_TRANSIT_TO|44|12557| 38.57138|   4641| -9.03899|  4641_0_2|  4641_0|            SCHEDULED|111500234560|      0.0| 150013|1737026203|4641_0_2|2700|102...|\n",
            "|   12.0|20250116-64010076...|    STOPPED_AT|44|12092| 38.70356|   4203|-8.953323|  4203_0_1|  4203_0|            SCHEDULED|121690234560|      0.0| 100038|1737026212|4203_0_1|2700|111...|\n",
            "|   97.0|20250116-64010042...|   INCOMING_AT|44|12746|38.701836|   4705| -8.97245|  4705_0_2|  4705_0|            SCHEDULED|123060234560|2.7777777| 100029|1737026206|4705_0_2|2700|110...|\n",
            "|  131.0|20250116-64010205...| IN_TRANSIT_TO|44|12731|38.550438|   4431|-8.882385|  4431_0_2|  4431_0|            SCHEDULED|112450234560|      0.0| 160529|1737026199|4431_0_2|2700|113...|\n",
            "|   67.0|20250116-64010146...| IN_TRANSIT_TO|44|12060|38.649464|   4604|-8.989128|  4604_0_1|  4604_0|            SCHEDULED|121010234560|      0.0| 090117|1737026201|4604_0_1|2700|104...|\n",
            "|   28.0|20250116-64010080...|   INCOMING_AT|44|12503|38.708984|   4514|-8.717798|  4514_0_1|  4514_0|            SCHEDULED|121650234560|15.833333| 130488|1737026207|4514_0_1|2700|103...|\n",
            "|  151.0|20250116-64010112...| IN_TRANSIT_TO|44|12506| 38.65653|   4600|-8.991332|  4600_0_1|  4600_0|            SCHEDULED|121340234560|      0.0| 090018|1737026205|4600_0_1|2700|103...|\n",
            "|  136.0|20250116-64010130...| IN_TRANSIT_TO|44|12515|38.720238|   4600|-8.999806|  4600_0_1|  4600_0|            SCHEDULED|121170234560|      5.0| 100283|1737026200|4600_0_1|2700|110...|\n",
            "|  200.0|20250116-64010164...| IN_TRANSIT_TO|44|12747|38.539406|   4720|-8.872938|  4720_0_1|  4720_0|            SCHEDULED|113250234560|     27.5| 160027|1737026198|4720_0_1|2700|104...|\n",
            "|  321.0|20250116-64010072...| IN_TRANSIT_TO|44|12633| 38.70059|   4513|-8.958016|  4513_0_2|  4513_0|            SCHEDULED|121720234560|13.611111| 100008|1737026211|4513_0_2|2700|110...|\n",
            "|  310.0|20250116-64010390...| IN_TRANSIT_TO|44|12742| 38.75371|   4702|-8.959135|  4702_0_1|  4702_0|            SCHEDULED|123040234560|      0.0| 010079|1737026205|4702_0_1|2700|105...|\n",
            "|   96.0|20250116-64010181...| IN_TRANSIT_TO|44|12673|38.539352|   4725| -8.88745|  4725_0_2|  4725_0|            SCHEDULED|113030234560| 9.722222| 162005|1737026201|4725_0_2|2700|111...|\n",
            "|    0.0|20250116-64010200...| IN_TRANSIT_TO|44|12085|38.522995|   4438|-8.895043|  4438_0_2|  4438_0|            SCHEDULED|112500234560|      0.0| 160105|1737026209|4438_0_2|2700|111...|\n",
            "|    0.0|       ESC_DU_EU1003|    STOPPED_AT| 43|2339|38.670578|   3505|-9.157463|  3505_0_2|  3505_0|            SCHEDULED|      EU1115|      0.0| 020533|1737026208|3505_0_2_1030_105...|\n",
            "|   83.0|20250116-64010167...| IN_TRANSIT_TO|44|12691| 38.52537|   4730|-8.887234|  4730_0_1|  4730_0|            SCHEDULED|113220234560|6.9444447| 160141|1737026201|4730_0_1|2700|100...|\n",
            "|   69.0|20250116-64010303...| IN_TRANSIT_TO|44|12586|38.580097|   4544|-8.722515|  4544_0_2|  4544_0|            SCHEDULED|111090234560|     20.0| 130630|1737026200|4544_0_2|2700|104...|\n",
            "|   25.0|           1_1739-11| IN_TRANSIT_TO| 41|1197| 38.78523|   1625|  -9.3438|  1625_1_1|  1625_1|            SCHEDULED|        1753| 9.722222| 171139|1737026199|1625_1_1_1030_105...|\n",
            "|   78.0|           1_1616-11| IN_TRANSIT_TO| 41|1315|38.756493|   1601|-9.263448|  1601_0_2|  1601_0|            SCHEDULED|        1602|12.222222| 170923|1737026203|1601_0_2_1030_105...|\n",
            "|  299.0|           1_1301-11| IN_TRANSIT_TO| 41|1400| 38.70076|   1614|-9.338386|  1614_1_1|  1614_1|            SCHEDULED|        1346|     10.0| 050163|1737026190|1614_1_1_1030_105...|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with tempfile.TemporaryDirectory() as d:\n",
        "    # Create a stream from the input data\n",
        "    df = (spark.readStream\n",
        "          .option(\"maxFilesPerTrigger\", 1)\n",
        "          .format(\"json\")\n",
        "          .schema(schema)  # Define the schema for the input data\n",
        "          .load(input_path))\n",
        "\n",
        "    # Apply transformation to the DataFrame\n",
        "    transformed_df = (df\n",
        "                       .withColumn(\"timestamp\", to_timestamp(from_unixtime(\"timestamp\"))))  # Example transformation\n",
        "\n",
        "    # Write the transformed data to the table\n",
        "    query = (transformed_df.writeStream(\n",
        "               \"vehicles_table\",\n",
        "\n",
        "            queryName='read_query',\n",
        "\n",
        "            outputMode=\"append\",\n",
        "\n",
        "            format='parquet',\n",
        "\n",
        "            checkpointLocation=d ) ) # Define the checkpoint location\n",
        "\n",
        "    # Sleep for a while to let the stream process some data\n",
        "    time.sleep(10)\n",
        "\n",
        "    # Stop the stream query after processing\n",
        "    query.stop()\n"
      ],
      "metadata": {
        "id": "PsbAcJ3t42Yf",
        "outputId": "d9dda9b4-9cec-4d8d-b5b6-4812f6417c51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'DataStreamWriter' object is not callable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-46290a660363>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Write the transformed data to the table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     query = (transformed_df.writeStream(\n\u001b[0m\u001b[1;32m     15\u001b[0m                \u001b[0;34m\"vehicles_table\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'DataStreamWriter' object is not callable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: convert the column timestamp to timestamp\n",
        "\n",
        "from pyspark.sql.functions import from_unixtime, to_timestamp\n"
      ],
      "metadata": {
        "id": "0GiwZmafzrzx"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: group by vehicle in 2 minutes buckets\n",
        "\n",
        "from pyspark.sql.functions import window, from_unixtime, col, to_timestamp\n",
        "\n",
        "# Assuming 'vehicles_table' is your table with the vehicle data\n",
        "\n",
        "# Read the data\n",
        "vehicle_data = spark.read.parquet(\"vehicles_table\")\n",
        "\n",
        "# Convert the timestamp to a timestamp type and adjust timezone\n",
        "vehicle_data = vehicle_data.withColumn(\"timestamp_readable\", to_timestamp(from_unixtime(col(\"timestamp\") / 1000)))\n",
        "\n",
        "# Group by vehicle and 2-minute intervals\n",
        "grouped_vehicle_data = vehicle_data.groupBy(\n",
        "    \"id\", window(\"timestamp_readable\", \"2 minutes\")\n",
        ").count()\n",
        "\n",
        "\n",
        "# Show the results\n",
        "grouped_vehicle_data.show(truncate=False)\n",
        "\n",
        "# Optional: Write the grouped data to a new location\n",
        "# grouped_vehicle_data.write.mode(\"overwrite\").parquet(\"grouped_vehicles_data\")"
      ],
      "metadata": {
        "id": "L6dm6YUuz_gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  spark.read.table(\"vehicles_table\").withColumn(\"timestamp\", to_timestamp(from_unixtime(\"timestamp\"))).show()"
      ],
      "metadata": {
        "id": "o11cpIq_WvXO",
        "outputId": "2bbee929-031b-4f97-9d1e-6171a27b72d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "|bearing|            block_id|current_status|      id|      lat|line_id|      lon|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id|          timestamp|             trip_id|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "|  183.0|20250116-64010165...| IN_TRANSIT_TO|44|12745| 38.76761|   4720|-9.100393|  4720_0_1|  4720_0|            SCHEDULED|113260234560|      0.0| 060011|2025-01-16 11:16:53|4720_0_1|2700|111...|\n",
            "|   58.0|20250116-64010273...| IN_TRANSIT_TO|44|12557| 38.57138|   4641| -9.03899|  4641_0_2|  4641_0|            SCHEDULED|111500234560|      0.0| 150013|2025-01-16 11:16:43|4641_0_2|2700|102...|\n",
            "|   12.0|20250116-64010076...|    STOPPED_AT|44|12092| 38.70356|   4203|-8.953323|  4203_0_1|  4203_0|            SCHEDULED|121690234560|      0.0| 100038|2025-01-16 11:16:52|4203_0_1|2700|111...|\n",
            "|   97.0|20250116-64010042...|   INCOMING_AT|44|12746|38.701836|   4705| -8.97245|  4705_0_2|  4705_0|            SCHEDULED|123060234560|2.7777777| 100029|2025-01-16 11:16:46|4705_0_2|2700|110...|\n",
            "|  131.0|20250116-64010205...| IN_TRANSIT_TO|44|12731|38.550438|   4431|-8.882385|  4431_0_2|  4431_0|            SCHEDULED|112450234560|      0.0| 160529|2025-01-16 11:16:39|4431_0_2|2700|113...|\n",
            "|   67.0|20250116-64010146...| IN_TRANSIT_TO|44|12060|38.649464|   4604|-8.989128|  4604_0_1|  4604_0|            SCHEDULED|121010234560|      0.0| 090117|2025-01-16 11:16:41|4604_0_1|2700|104...|\n",
            "|   28.0|20250116-64010080...|   INCOMING_AT|44|12503|38.708984|   4514|-8.717798|  4514_0_1|  4514_0|            SCHEDULED|121650234560|15.833333| 130488|2025-01-16 11:16:47|4514_0_1|2700|103...|\n",
            "|  151.0|20250116-64010112...| IN_TRANSIT_TO|44|12506| 38.65653|   4600|-8.991332|  4600_0_1|  4600_0|            SCHEDULED|121340234560|      0.0| 090018|2025-01-16 11:16:45|4600_0_1|2700|103...|\n",
            "|  136.0|20250116-64010130...| IN_TRANSIT_TO|44|12515|38.720238|   4600|-8.999806|  4600_0_1|  4600_0|            SCHEDULED|121170234560|      5.0| 100283|2025-01-16 11:16:40|4600_0_1|2700|110...|\n",
            "|  200.0|20250116-64010164...| IN_TRANSIT_TO|44|12747|38.539406|   4720|-8.872938|  4720_0_1|  4720_0|            SCHEDULED|113250234560|     27.5| 160027|2025-01-16 11:16:38|4720_0_1|2700|104...|\n",
            "|  321.0|20250116-64010072...| IN_TRANSIT_TO|44|12633| 38.70059|   4513|-8.958016|  4513_0_2|  4513_0|            SCHEDULED|121720234560|13.611111| 100008|2025-01-16 11:16:51|4513_0_2|2700|110...|\n",
            "|  310.0|20250116-64010390...| IN_TRANSIT_TO|44|12742| 38.75371|   4702|-8.959135|  4702_0_1|  4702_0|            SCHEDULED|123040234560|      0.0| 010079|2025-01-16 11:16:45|4702_0_1|2700|105...|\n",
            "|   96.0|20250116-64010181...| IN_TRANSIT_TO|44|12673|38.539352|   4725| -8.88745|  4725_0_2|  4725_0|            SCHEDULED|113030234560| 9.722222| 162005|2025-01-16 11:16:41|4725_0_2|2700|111...|\n",
            "|    0.0|20250116-64010200...| IN_TRANSIT_TO|44|12085|38.522995|   4438|-8.895043|  4438_0_2|  4438_0|            SCHEDULED|112500234560|      0.0| 160105|2025-01-16 11:16:49|4438_0_2|2700|111...|\n",
            "|    0.0|       ESC_DU_EU1003|    STOPPED_AT| 43|2339|38.670578|   3505|-9.157463|  3505_0_2|  3505_0|            SCHEDULED|      EU1115|      0.0| 020533|2025-01-16 11:16:48|3505_0_2_1030_105...|\n",
            "|   83.0|20250116-64010167...| IN_TRANSIT_TO|44|12691| 38.52537|   4730|-8.887234|  4730_0_1|  4730_0|            SCHEDULED|113220234560|6.9444447| 160141|2025-01-16 11:16:41|4730_0_1|2700|100...|\n",
            "|   69.0|20250116-64010303...| IN_TRANSIT_TO|44|12586|38.580097|   4544|-8.722515|  4544_0_2|  4544_0|            SCHEDULED|111090234560|     20.0| 130630|2025-01-16 11:16:40|4544_0_2|2700|104...|\n",
            "|   25.0|           1_1739-11| IN_TRANSIT_TO| 41|1197| 38.78523|   1625|  -9.3438|  1625_1_1|  1625_1|            SCHEDULED|        1753| 9.722222| 171139|2025-01-16 11:16:39|1625_1_1_1030_105...|\n",
            "|   78.0|           1_1616-11| IN_TRANSIT_TO| 41|1315|38.756493|   1601|-9.263448|  1601_0_2|  1601_0|            SCHEDULED|        1602|12.222222| 170923|2025-01-16 11:16:43|1601_0_2_1030_105...|\n",
            "|  299.0|           1_1301-11| IN_TRANSIT_TO| 41|1400| 38.70076|   1614|-9.338386|  1614_1_1|  1614_1|            SCHEDULED|        1346|     10.0| 050163|2025-01-16 11:16:30|1614_1_1_1030_105...|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.table(\"vehicles_table\").write.format('bigquery') \\\n",
        "  .option('table', output_table) \\\n",
        "  .option(\"createDisposition\", \"CREATE_IF_NEEDED\") \\\n",
        "  .option(\"temporaryGcsBucket\", \"edit-data-eng-project-group3/streaming_data/temp-bucket\") \\\n",
        "  .mode(\"overwrite\")\n",
        "  .save()"
      ],
      "metadata": {
        "id": "rKsstA8EYIjt",
        "outputId": "57abe6cc-433d-4856-e0a2-dc959b960818",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o274.save.\n: com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:110)\n\tat com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Request couldn't be served.\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:220)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:405)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:394)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryRetryHelper.run(BigQueryRetryHelper.java:86)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryRetryHelper.runWithRetries(BigQueryRetryHelper.java:49)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:393)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:358)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:328)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:323)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:564)\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.java:134)\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:107)\n\t... 44 more\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found\nPOST https://www.googleapis.com/bigquery/v2/projects//jobs?prettyPrint=false\n{\n  \"code\": 404,\n  \"errors\": [\n    {\n      \"domain\": \"global\",\n      \"message\": \"Request couldn't be served.\",\n      \"reason\": \"notFound\"\n    }\n  ],\n  \"message\": \"Request couldn't be served.\",\n  \"status\": \"NOT_FOUND\"\n}\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$3.interceptResponse(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:552)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:493)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:603)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:218)\n\t... 56 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-20392059fc2b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temporaryGcsBucket\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"edit-data-eng-project-group3/streaming_data/temp-bucket\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o274.save.\n: com.google.cloud.bigquery.connector.common.BigQueryConnectorException: Failed to write to BigQuery\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:110)\n\tat com.google.cloud.spark.bigquery.write.BigQueryDeprecatedIndirectInsertableRelation.insert(BigQueryDeprecatedIndirectInsertableRelation.java:43)\n\tat com.google.cloud.spark.bigquery.write.CreatableRelationProviderHelper.createRelation(CreatableRelationProviderHelper.java:54)\n\tat com.google.cloud.spark.bigquery.BigQueryRelationProvider.createRelation(BigQueryRelationProvider.scala:107)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:110)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:106)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:82)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:481)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:457)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:106)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:93)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:91)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:128)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:848)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:382)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:355)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryException: Request couldn't be served.\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.translate(HttpBigQueryRpc.java:115)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:220)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:405)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl$5.call(BigQueryImpl.java:394)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.gax.retrying.DirectRetryingExecutor.submit(DirectRetryingExecutor.java:103)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryRetryHelper.run(BigQueryRetryHelper.java:86)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryRetryHelper.runWithRetries(BigQueryRetryHelper.java:49)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:393)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.BigQueryImpl.create(BigQueryImpl.java:358)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:328)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.createAndWaitFor(BigQueryClient.java:323)\n\tat com.google.cloud.bigquery.connector.common.BigQueryClient.loadDataIntoTable(BigQueryClient.java:564)\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.loadDataToBigQuery(BigQueryWriteHelper.java:134)\n\tat com.google.cloud.spark.bigquery.write.BigQueryWriteHelper.writeDataFrameToBigQuery(BigQueryWriteHelper.java:107)\n\t... 44 more\nCaused by: com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException: 404 Not Found\nPOST https://www.googleapis.com/bigquery/v2/projects//jobs?prettyPrint=false\n{\n  \"code\": 404,\n  \"errors\": [\n    {\n      \"domain\": \"global\",\n      \"message\": \"Request couldn't be served.\",\n      \"reason\": \"notFound\"\n    }\n  ],\n  \"message\": \"Request couldn't be served.\",\n  \"status\": \"NOT_FOUND\"\n}\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.json.GoogleJsonResponseException.from(GoogleJsonResponseException.java:146)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:118)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.json.AbstractGoogleJsonClientRequest.newExceptionOnError(AbstractGoogleJsonClientRequest.java:37)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest$3.interceptResponse(AbstractGoogleClientRequest.java:466)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.http.HttpRequest.execute(HttpRequest.java:1111)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:552)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.executeUnparsed(AbstractGoogleClientRequest.java:493)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.api.client.googleapis.services.AbstractGoogleClientRequest.execute(AbstractGoogleClientRequest.java:603)\n\tat com.google.cloud.spark.bigquery.repackaged.com.google.cloud.bigquery.spi.v2.HttpBigQueryRpc.create(HttpBigQueryRpc.java:218)\n\t... 56 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dd1mGZxai4bB"
      },
      "source": [
        "# Step 7: Write streaming data to BigQuery with auto-table creation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jkzBu8EBi4bB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        },
        "outputId": "45f490ea-0d6a-48ca-d931-6bf6ecdc79c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/socket.py\", line 718, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9f008dff6c87>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mstreaming_query\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/streaming.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "streaming_query = streaming_df.writeStream \\\n",
        "    .format(\"bigquery\") \\\n",
        "    .option(\"table\", output_table) \\\n",
        "    .option(\"checkpointLocation\", \"gs://edit-data-eng-project-group3/streaming_data/checkpoints\") \\\n",
        "    .option(\"temporaryGcsBucket\", \"gs://edit-data-eng-project-group3/streaming_data/temp-bucket\") \\\n",
        "    .option(\"writeDisposition\", \"WRITE_APPEND\") \\\n",
        "    .option(\"createDisposition\", \"CREATE_IF_NEEDED\") \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .start()\n",
        "\n",
        "streaming_query.awaitTermination()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "streaming_query.stop()"
      ],
      "metadata": {
        "id": "0U3QqQwuTGVX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZKbyGnOSSmxE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}